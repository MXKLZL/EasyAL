# -*- coding: utf-8 -*-
"""query_strategy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_G_vtAh9NwFoSrDjE_ZsY_2xTij10rxy
"""
import numpy as np
import torch
import time
from sklearn.cluster import KMeans
from tqdm.notebook import tqdm

def query(strategy, model_class, label_per_round):
  start = time.time()

  if strategy == 'random':
    unlabel_index = model_class.get_unlabeled_index()
    np.random.shuffle(unlabel_index)

    end = time.time()
    duration = end - start
    return duration, unlabel_index[:label_per_round]

  if strategy == 'uncertain':
    unlabel_index = model_class.get_unlabeled_index()
    p, _ = torch.max(model_class.predict_unlabeled(), 1)
    selected_index = p.numpy().argsort()[:label_per_round]

    end = time.time()
    duration = end - start
    return duration, unlabel_index[selected_index]

  if strategy == 'margin':
    unlabel_index = model_class.get_unlabeled_index()
    p = model_class.predict_unlabeled()
    p = -np.sort(-p, axis=1)
    # sort every row by descending
    diff = p[:, 0] - p[:, 1]
    # get difference between first class and second class
    selected_index = np.argsort(diff)[:label_per_round]

    end = time.time()
    duration = end - start
    return duration, unlabel_index[selected_index]

  if strategy == 'entropy':
    unlabel_index = model_class.get_unlabeled_index()
    p = model_class.predict_unlabeled()
    entropy = (-p * torch.log(p)).sum(1)
    # calculate entropy for each image
    selected_index = np.argsort(entropy.numpy())[::-1][:label_per_round]

    end = time.time()
    duration = end - start
    return duration, unlabel_index[selected_index]

  if strategy == 'k_means':
    unlabel_index = model_class.get_unlabeled_index()
    embedding = np.array(model_class.get_embedding_unlabeled())
    cluster_ = KMeans(n_clusters=label_per_round)
    cluster_.fit(embedding)
    cluster_index = cluster_.predict(embedding)
    centers = cluster_.cluster_centers_[cluster_index]
    dis = np.sum(np.array(pow((embedding - centers), 2)), axis=1)

    centerlabels = []
    for i in range(label_per_round):
      clusterlabel = np.where(cluster_index == i)[0]
      centerlabels.append(clusterlabel[dis[clusterlabel].argsort()[0]])

    end = time.time()
    duration = end - start
    return duration, unlabel_index[centerlabels]

  if strategy == 'k_center_greedy':
    # unlabel_index = model_class.get_unlabeled_index()
    # unlabel_embedding = np.array(model_class.get_embedding_unlabeled())
    # label_index = model_class.labeled_index
    # label_embedding = np.array(model_class.get_embedding(model_class.data_loader_labeled))

    # # for each unlabeled dataset i, compute its distance with all labeled j, 
    # # and find the smallest distance
    # min_dists = []
    # for i in range(len(unlabel_embedding)):
    #   l2_dists = np.linalg.norm(unlabel_embedding[i] - label_embedding, axis=1)
    #   min_dists.append(l2_dists.min())

    # selected_index = np.argsort(min_dists)[::-1][:label_per_round]
    # # find the unlabeled dataset i with largest minimal distance 
    # end = time.time()
    # duration = end - start
    # return duration, unlabel_index[selected_index]

    unlabel_index = model_class.get_unlabeled_index()
    unlabel_embedding = np.array(model_class.get_embedding_unlabeled())
    label_embedding = np.array(model_class.get_embedding(model_class.data_loader_labeled))
    batch = []
    for j in tqdm(range(label_per_round)):
      min_dists = []
      for i in range(len(unlabel_embedding)):
        #print(unlabel_embedding[i] - label_embedding)
        l2_dists = np.linalg.norm(unlabel_embedding[i] - label_embedding, axis=1)
        #print(l2_dists)
        min_dists.append(l2_dists.min())

      #get index of data we choose in unlabel idx array
      label_greedy = np.argsort(min_dists)[::-1][0]

      #update embedding of label and unlabel data
      label_greedy_embedding = unlabel_embedding[label_greedy]
      unlabel_embedding = np.delete(unlabel_embedding, label_greedy,0)
      label_embedding = np.append(label_embedding, [label_greedy_embedding],axis = 0) 

      #update index of label and unlabel data
      label_greedy_idx = unlabel_index[label_greedy]
      unlabel_index = np.delete(unlabel_index, label_greedy)

      #update result
      batch.append(label_greedy_idx)

    end = time.time()
    duration = end - start
    return duration, np.array(batch)
  
  if strategy == 'confident_coreset':

    unlabel_index = model_class.get_unlabeled_index()
    unlabel_embedding = np.array(model_class.get_embedding_unlabeled())
    unlabel_loss = np.array(model_class.get_uncertainty().view(1,-1))
    label_embedding = np.array(model_class.get_embedding(model_class.data_loader_labeled))
    batch = []
    for j in tqdm(range(label_per_round)):
      min_dists = []
      for i in range(len(unlabel_embedding)):
        #print(unlabel_embedding[i] - label_embedding)
        l2_dists = np.linalg.norm(unlabel_embedding[i] - label_embedding, axis=1)
        #print(l2_dists)
        min_dists.append(l2_dists.min())


      #get index of data we choose in unlabel idx array
      label_greedy = np.argsort(min_dists*unlabel_loss.squeeze())[::-1][0]

      #update embedding of label and unlabel data
      label_greedy_embedding = unlabel_embedding[label_greedy]
      unlabel_embedding = np.delete(unlabel_embedding, label_greedy,0)
      unlabel_loss = np.delete(unlabel_loss, label_greedy,1)
      label_embedding = np.append(label_embedding, [label_greedy_embedding],axis = 0) 

      #update index of label and unlabel data
      label_greedy_idx = unlabel_index[label_greedy]
      unlabel_index = np.delete(unlabel_index, label_greedy)

      #update result
      batch.append(label_greedy_idx)

    end = time.time()
    duration = end - start
    return duration, np.array(batch)
